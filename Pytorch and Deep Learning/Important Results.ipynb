{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.ToTensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 200\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),  # scale imported image\n",
    "    transforms.ToTensor()])\n",
    "im1 = Image.open(\"/home/kush/Desktop/silverback.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 200, 297])\n",
      "torch.Size([3, 673, 1000])\n"
     ]
    }
   ],
   "source": [
    "ImgProc1 = transforms.ToTensor() # Processing 1\n",
    "\n",
    "ImgProc2 =loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),  \n",
    "    transforms.ToTensor()]) # All Processes in order\n",
    "\n",
    "image1 = ImgProc2(im1) # First image processed (should be PIL)\n",
    "\n",
    "print type(image1)     # image1 is Tensor\n",
    "\n",
    "image2 = ImgProc1(im1) # Second \n",
    "\n",
    "ImgConv = transforms.ToPILImage() # Converter object for tensors\n",
    "\n",
    "PIL_im1 = ImgConv(image1) # First Tensor converted to PIL\n",
    "PIL_im2 = ImgConv(image2)\n",
    "\n",
    "print image1.size() # new size due to Resize is 200*new_width\n",
    "print image2.size() # orignal size \n",
    "\n",
    "# PIL_im1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4])\n",
    "print a.size()\n",
    "a = a.view(2,-1)\n",
    "print a.size()\n",
    "a = a.unsqueeze(1) # adds dummy dimension at index 1\n",
    "print a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Basics\n",
    "x1 = torch.tensor([1], requires_grad=True)\n",
    "# x1.requires_grad_(requires_grad=True) # if not already set\n",
    "x2 = torch.tensor([3], requires_grad = True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#########################################################\n",
    "#########################################################\n",
    "CASE WHEN retain_graph is False\n",
    "#########################################################\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x1.grad is not None:\n",
    "    x1.grad.zero_()      # first time x1.grad will be None so skip ,\n",
    "                         # next time it is important to make grads 0\n",
    "                         # and then redifine whole graph again\n",
    "\n",
    "if x2.grad is not None:  # Do this for both leaf nodes  \n",
    "    x2.grad.zero_()\n",
    "    \n",
    "    \n",
    "y = 2*(x1**2) + x2**2\n",
    "z = y**3\n",
    "w = z.sum()\n",
    "# Two leaf nodes x1 and x2\n",
    "# No other intermediate node can have req_grad = True\n",
    "w.backward()\n",
    "\n",
    "# IF retain_graph == False\n",
    "# our intermediate values of y,z and w get flushed to free up space\n",
    "# we can not call w.backward again , as it does not have resources \n",
    "# to calculate it .\n",
    "# Therefore , we will need to redifine the whole graph again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1452])\n",
      "tensor([ 2178])\n"
     ]
    }
   ],
   "source": [
    "print x1.grad\n",
    "print x2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################\n",
    "#########################################################\n",
    "CASE WHEN retain_graph is False\n",
    "#########################################################\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor([1], requires_grad=True)\n",
    "# x1.requires_grad_(requires_grad=True) # if not already set\n",
    "x2 = torch.tensor([3], requires_grad = True)\n",
    "\n",
    "y = 2*(x1**2) + x2**2\n",
    "z = y**3\n",
    "w = z.sum()\n",
    "# Two leaf nodes x1 and x2\n",
    "# No other intermediate node can have req_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1452])\n",
      "tensor([ 2178])\n"
     ]
    }
   ],
   "source": [
    "if x1.grad is not None:\n",
    "    x1.grad.zero_()      # first time x1.grad will be None so skip ,\n",
    "                         # next time it is important to make grads 0\n",
    "                         # and then redifine whole graph again\n",
    "\n",
    "if x2.grad is not None:  # Do this for both leaf nodes  \n",
    "    x2.grad.zero_()\n",
    "\n",
    "w.backward(retain_graph = True)\n",
    "\n",
    "print x1.grad\n",
    "print x2.grad\n",
    "\n",
    "# Here on making retain_graph = True \n",
    "# I didn't need to remake/ recalculate graph\n",
    "# Additionally , I can comment x2.grad and x1.grad if i wish to accumalate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CASE 1\n",
    "x = torch.tensor([3],requires_grad=True)\n",
    "y = x\n",
    "z = y**2\n",
    "\n",
    "z.backward()\n",
    "print y.grad # y is just another name for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 18])\n",
      "tensor([ 108])\n"
     ]
    }
   ],
   "source": [
    "# CASE 2 :retain_grad()\n",
    "x = torch.tensor([3],requires_grad=True)\n",
    "y = x**2\n",
    "z = y**2\n",
    "y.retain_grad()\n",
    "z.backward()\n",
    "\n",
    "print y.grad # dz/dy\n",
    "print x.grad # dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20])\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]])\n",
      "##############################\n",
      "weights\n",
      "tensor([[-0.0857,  0.8062,  0.2587],\n",
      "        [-0.0857,  0.8062,  0.2587],\n",
      "        [-0.0857,  0.8062,  0.2587]])\n",
      "##############################\n",
      "biases\n",
      "tensor([[-0.2029, -0.2079, -0.0282],\n",
      "        [-0.2029, -0.2079, -0.0282],\n",
      "        [-0.2029, -0.2079, -0.0282]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(10,20) \n",
    "m = torch.nn.Linear(in_features= 20, out_features= 30 , bias = False)\n",
    "# m is a matrix of size 20*30 with random values \n",
    "print x.size()\n",
    "print m(x)[:3,:3] # All zeros with bias = False\n",
    "                  \n",
    "# If we wanted to find initial weights of all edges\n",
    "x = x + 1\n",
    "print '#'*30\n",
    "print \"weights\"\n",
    "print m(x)[:3, :3]\n",
    "# If we wanted to know initial weight of biases\n",
    "x = x - 1\n",
    "m = torch.nn.Linear(in_features= 20, out_features= 30 , bias = True)\n",
    "print '#'*30\n",
    "print \"biases\"\n",
    "print m(x)[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([ 6])\n"
     ]
    }
   ],
   "source": [
    "# CASE 2 :retain_grad()\n",
    "x = torch.tensor([3],requires_grad=True)\n",
    "y = x.clone()\n",
    "z = y**2\n",
    "q = x**2\n",
    "y.retain_grad()\n",
    "q.backward()\n",
    "\n",
    "print y.grad # dz/dy\n",
    "print x.grad # dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.vgg16(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# for i in cnn:\n",
    "#     print isinstance(i,torch.nn.modules.conv.Conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = F.softmax(self.out(x))\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=2, n_hidden=10, n_output=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5196, -0.3155],\n",
      "        [ 0.0001, -0.0907],\n",
      "        [-0.3217, -0.2253],\n",
      "        [ 0.3643, -0.0637],\n",
      "        [-0.4628, -0.4719],\n",
      "        [-0.2935,  0.5907],\n",
      "        [ 0.0572, -0.6557],\n",
      "        [-0.3555, -0.0312],\n",
      "        [-0.5306,  0.4327],\n",
      "        [-0.3127,  0.2162]])\n",
      "Parameter containing:\n",
      "tensor([-0.5295, -0.6759, -0.5870, -0.6399,  0.3817,  0.3991, -0.3413,\n",
      "         0.3378,  0.3137, -0.2684])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0123,  0.2982, -0.0666, -0.0272, -0.0792, -0.2497, -0.2191,\n",
      "          0.2867, -0.2097, -0.2671],\n",
      "        [-0.3132,  0.2973,  0.0714,  0.3036,  0.2546, -0.3048,  0.2165,\n",
      "          0.1221,  0.0846, -0.1101]])\n",
      "Parameter containing:\n",
      "tensor([ 0.1184,  0.2469])\n",
      "<bound method Net.parameters of Net(\n",
      "  (hidden): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=2, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "for i in net.parameters():\n",
    "    print i\n",
    "print net.parameters\n",
    "# prints 4 matrices\n",
    "\n",
    "# 1st = 10 * 2 --> 2 input layer * 10 hidden\n",
    "# 2nd = 1 * 10 --> 1 bias for each node in hidden layer\n",
    "# 3rd = 10 * 2 --> 10 input hidden layer * 2 output layer\n",
    "# 4th = 1 * 2 --> 1 bias for each node output layer\n",
    "\n",
    "#TODO : Read about instancemethod\n",
    "#TODO : Study bptt( back prop through time ) to \n",
    "#       understand why we need to make optimizer zero_() here\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MSELoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"c\" + str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2], requires_grad = True )\n",
    "y = x**2\n",
    "p = y.detach()\n",
    "q = y.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<PowBackward0 object at 0x7fa30b8b5310>\n",
      "None\n",
      "<CloneBackward object at 0x7fa30b8b5310>\n"
     ]
    }
   ],
   "source": [
    "print x.grad_fn\n",
    "print y.grad_fn\n",
    "print p.grad_fn\n",
    "print q.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"/home/kush/Downloads/mnist_train.csv\").values\n",
    "#  .values imports it as ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape    \n",
    "y = ds[:,0]     \n",
    "im_np = ds[:,1:785]    \n",
    "im_np = np.reshape(im_np,(-1,28,28)) \n",
    "im_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(im_np[0,:,:].astype('uint8') )\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff9c7fc15d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ix in im_np[0:10,:,:]:\n",
    "    plt.imshow(ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
